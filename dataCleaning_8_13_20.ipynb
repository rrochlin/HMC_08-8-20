{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "5751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n",
    "from sklearn.cluster import KMeans\n",
    "import time as clock\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = clock.time()"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = glob.glob(\"./Data/*.csv\")\n",
    "# insert the desired start time\n",
    "cutOffTime = pd.Timestamp(\"8-8-20 10:30\")\n",
    "endTime = pd.Timestamp(\"8-8-20 14:00\")\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "# sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "# columns = [0,1,6,7,8,9,10,11]\n",
    "# Enable Data Checking\n",
    "DataChecking = False\n",
    "# Here are obversed timestamps that need to removed from the data\n",
    "# badTimes = ['     0/0/0      0:0:0','2165/165/165 165:165:85']\n",
    "# Controls wether zones will be created automatically or by k-means clusters\n",
    "ZoneAutomation = False\n",
    "# Sets either the binning or the manual zones\n",
    "numberOfZones = 4\n",
    "# Sensors to exclude from zone\n",
    "outdoorSensors = []#['S-15','S-16','S-18','S-19']\n",
    "# 10s of seconds before nebulization to include in the expirement csv files\n",
    "preCursorFactor = 6\n",
    "# which particle to analyze\n",
    "particle = 'Dp>0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "expTRange = {\n",
    "    'HMC Expirement 1': [\n",
    "    # pd.Timestamp(\"2020-08-08 11:22:51\"),\n",
    "    pd.Timestamp(\"2020-08-08 11:36:01\"),\n",
    "    pd.Timestamp(\"2020-08-08 11:52:01\")],\n",
    "    'HMC Expirement 2': [\n",
    "    pd.Timestamp(\"2020-08-08 12:00:11\"),\n",
    "    pd.Timestamp(\"2020-08-08 12:11:01\")],\n",
    "    # pd.Timestamp(\"2020-08-08 12:20:51\")],\n",
    "    'HMC Expirement 3': [\n",
    "    pd.Timestamp(\"2020-08-08 12:20:51\"),\n",
    "    pd.Timestamp(\"2020-08-08 12:36:01\"),\n",
    "    pd.Timestamp(\"2020-08-08 12:51:21\"),\n",
    "    pd.Timestamp(\"2020-08-08 13:06:01\")]\n",
    "\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'HMC Expirement 1' : 8*6,\n",
    "    'HMC Expirement 2' : 8*6,\n",
    "    'HMC Expirement 3' : 14*6\n",
    "}\n",
    "# Manual Zone set up notice how we are missing S-14\n",
    "zoneList = {\n",
    "    'Zone 1' : ['B-10'],\n",
    "    'Zone 2' : ['B-17','B-14','B-15','B-07','B-08'],\n",
    "    'Zone 3' : ['B-01','B-02','B-13','B-18','B-23','B-04','B-05','B-06','B-11','B-12','B-21','B-22','B-16','B-09','B-19','B-20']\n",
    "}\n",
    "\n",
    "if not ZoneAutomation:\n",
    "    numberOfZones = len(zoneList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HMC Expirement 1 2020-08-08 11:36:01\nHMC Expirement 1 2020-08-08 11:52:01\nHMC Expirement 2 2020-08-08 12:00:11\nHMC Expirement 2 2020-08-08 12:11:01\nHMC Expirement 3 2020-08-08 12:20:51\nHMC Expirement 3 2020-08-08 12:36:01\nHMC Expirement 3 2020-08-08 12:51:21\nHMC Expirement 3 2020-08-08 13:06:01\n"
     ]
    }
   ],
   "source": [
    "for i in expTRange:\n",
    "        for time in expTRange[i]:\n",
    "            print(i,time)"
   ]
  },
  {
   "source": [
    "Removed file 13 here for some reason"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_csv_files.pop(11)"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for x in all_csv_files:\n",
    "    if not (x == './Data\\\\B-02.csv'):\n",
    "        name = x.split('\\\\')[1].split('.')[0]\n",
    "        data[name] = pd.read_csv(x, parse_dates=[[0,2]])\n",
    "        data[name] = data[name][data[name].columns[[0,6]]]\n",
    "    else:\n",
    "        name = x.split('\\\\')[1].split('.')[0]\n",
    "        data[name] = pd.read_csv(x, parse_dates=[[0,1]],usecols=[0,1,6])\n"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Checking Flag is False, no tests ran\n"
     ]
    }
   ],
   "source": [
    "if DataChecking:\n",
    "    directory = './dataInfo'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "    errors = {}\n",
    "    errorCount = {}\n",
    "    # Enter the expected interval here\n",
    "    interval = 10\n",
    "    for x in data:\n",
    "        # errors keeps track of length of each time interval error that occurs\n",
    "        errors[x] = set(())\n",
    "        # errorCount keeps track of how many times each time interval error occured\n",
    "        errorCount[x] = {}\n",
    "        # counter keeps track of the total time interval errors per sensor\n",
    "        counter = 0\n",
    "        #shows the total\n",
    "        temp = data[x]\n",
    "        for idx,i in enumerate(temp['Date_Time']):\n",
    "            try:\n",
    "                if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                    timeErr = temp['Date_Time'][idx+1] - i\n",
    "                    if str(timeErr.seconds) in errorCount[x]:\n",
    "                        errorCount[x][str(timeErr.seconds)] +=1\n",
    "                    else:\n",
    "                        errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                    errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "        fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "        # display the different types of errors\n",
    "        lst = [i.seconds for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        # display the quantity of each type of error\n",
    "        lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        print()\n",
    "        fout.write('\\n')\n",
    "\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "else:\n",
    "    print(\"Data Checking Flag is False, no tests ran\")"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "B-01   ['% of values from interpolation : 5.775', '% of values from 0-padding : 9.721', '% of values not changed : 84.504']\n",
      "B-02   ['% of values from interpolation : 32.724', '% of values from 0-padding : 2.502', '% of values not changed : 64.774']\n",
      "B-04   ['% of values from interpolation : 33.812', '% of values from 0-padding : 3.736', '% of values not changed : 62.452']\n",
      "B-05   ['% of values from interpolation : 13.404', '% of values from 0-padding : 9.45', '% of values not changed : 77.146']\n",
      "B-06   ['% of values from interpolation : 32.56', '% of values from 0-padding : 3.382', '% of values not changed : 64.058']\n",
      "B-07   ['% of values from interpolation : 12.692', '% of values from 0-padding : 12.885', '% of values not changed : 74.423']\n",
      "B-08   ['% of values from interpolation : 7.781', '% of values from 0-padding : 12.872', '% of values not changed : 79.347']\n",
      "B-09   ['% of values from interpolation : 0.0', '% of values from 0-padding : 12.066', '% of values not changed : 87.934']\n",
      "B-10   ['% of values from interpolation : 13.052', '% of values from 0-padding : 14.395', '% of values not changed : 72.553']\n",
      "B-11   ['% of values from interpolation : 2.896', '% of values from 0-padding : 8.88', '% of values not changed : 88.224']\n",
      "B-12   ['% of values from interpolation : 32.722', '% of values from 0-padding : 2.413', '% of values not changed : 64.865']\n",
      "B-13   ['% of values from interpolation : 32.375', '% of values from 0-padding : 2.107', '% of values not changed : 65.517']\n",
      "B-14   ['% of values from interpolation : 1.834', '% of values from 0-padding : 10.618', '% of values not changed : 87.548']\n",
      "B-15   ['% of values from interpolation : 31.352', '% of values from 0-padding : 6.52', '% of values not changed : 62.128']\n",
      "B-16   ['% of values from interpolation : 2.507', '% of values from 0-padding : 16.779', '% of values not changed : 80.714']\n",
      "B-17   ['% of values from interpolation : 1.739', '% of values from 0-padding : 10.821', '% of values not changed : 87.44']\n",
      "B-18   ['% of values from interpolation : 1.159', '% of values from 0-padding : 10.242', '% of values not changed : 88.599']\n",
      "B-19   ['% of values from interpolation : 24.785', '% of values from 0-padding : 25.455', '% of values not changed : 49.761']\n",
      "B-20   ['% of values from interpolation : 2.122', '% of values from 0-padding : 11.861', '% of values not changed : 86.017']\n",
      "B-21   ['% of values from interpolation : 2.703', '% of values from 0-padding : 29.44', '% of values not changed : 67.857']\n",
      "B-22   ['% of values from interpolation : 33.27', '% of values from 0-padding : 4.671', '% of values not changed : 62.059']\n",
      "B-23   ['% of values from interpolation : 2.111', '% of values from 0-padding : 11.228', '% of values not changed : 86.66']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 1035\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "lowIDX,lowValue = [[i,value] for i,value in enumerate(length) if value == index][0]\n",
    "print(lowIDX,lowValue)"
   ]
  },
  {
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date_Time  B-01  B-02  B-04  B-05  B-06  B-07  B-08  B-09  \\\n",
       "0    2020-08-08 10:30:00     0     0     0     0     0     0     0     0   \n",
       "1    2020-08-08 10:30:10     0     0     0     0     0     0     0     0   \n",
       "2    2020-08-08 10:30:20     0     0     0     0     0     0     0     0   \n",
       "3    2020-08-08 10:30:30     0     0     0     0     0     0     0     0   \n",
       "4    2020-08-08 10:30:40     0     0     0     0     0     0     0     0   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1030 2020-08-08 13:21:40     0     0    18     0     0     0     0     0   \n",
       "1031 2020-08-08 13:21:50     0     0    18     0     0     0     0    18   \n",
       "1032 2020-08-08 13:22:00     0     0    18    21     0     0     0    81   \n",
       "1033 2020-08-08 13:22:10     0     0     9    21     0     0     0   114   \n",
       "1034 2020-08-08 13:22:20     0     0     0     0     0     0     0   141   \n",
       "\n",
       "      B-10  ...  B-16  B-17  B-18  B-19  B-20  B-21  B-22  B-23    Average  \\\n",
       "0        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "1        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "2        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "3        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "4        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "...    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...        ...   \n",
       "1030     0  ...    69     0     0     0     0     9     0     0   9.545455   \n",
       "1031     0  ...    69     0     0     0     0     9     0     0   9.818182   \n",
       "1032     0  ...     9     0     0     0     0     0     4     0  10.454545   \n",
       "1033     9  ...     9     0     0     0     0     9     9     0  14.227273   \n",
       "1034     9  ...     0     0     0     4     0    18     0     9  18.454545   \n",
       "\n",
       "         Variance  \n",
       "0        0.000000  \n",
       "1        0.000000  \n",
       "2        0.000000  \n",
       "3        0.000000  \n",
       "4        0.000000  \n",
       "...           ...  \n",
       "1030   648.520661  \n",
       "1031   549.966942  \n",
       "1032   621.975207  \n",
       "1033  1230.811983  \n",
       "1034  2887.066116  \n",
       "\n",
       "[1035 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date_Time</th>\n      <th>B-01</th>\n      <th>B-02</th>\n      <th>B-04</th>\n      <th>B-05</th>\n      <th>B-06</th>\n      <th>B-07</th>\n      <th>B-08</th>\n      <th>B-09</th>\n      <th>B-10</th>\n      <th>...</th>\n      <th>B-16</th>\n      <th>B-17</th>\n      <th>B-18</th>\n      <th>B-19</th>\n      <th>B-20</th>\n      <th>B-21</th>\n      <th>B-22</th>\n      <th>B-23</th>\n      <th>Average</th>\n      <th>Variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-08-08 10:30:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-08-08 10:30:10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-08-08 10:30:20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-08-08 10:30:30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-08-08 10:30:40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1030</th>\n      <td>2020-08-08 13:21:40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.545455</td>\n      <td>648.520661</td>\n    </tr>\n    <tr>\n      <th>1031</th>\n      <td>2020-08-08 13:21:50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>...</td>\n      <td>69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.818182</td>\n      <td>549.966942</td>\n    </tr>\n    <tr>\n      <th>1032</th>\n      <td>2020-08-08 13:22:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>81</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>10.454545</td>\n      <td>621.975207</td>\n    </tr>\n    <tr>\n      <th>1033</th>\n      <td>2020-08-08 13:22:10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>114</td>\n      <td>9</td>\n      <td>...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>14.227273</td>\n      <td>1230.811983</td>\n    </tr>\n    <tr>\n      <th>1034</th>\n      <td>2020-08-08 13:22:20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>141</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>9</td>\n      <td>18.454545</td>\n      <td>2887.066116</td>\n    </tr>\n  </tbody>\n</table>\n<p>1035 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "columns = list(interpDF.keys())\n",
    "mergedData = pd.DataFrame({'Date_Time':interpDF[columns[lowIDX]]['Date_Time']})\n",
    "for idx,column in enumerate(columns):\n",
    "    mergedData[column] = interpDF[column][particle]\n",
    "Average = np.mean(mergedData,axis=1)\n",
    "Variance = np.var(mergedData,axis=1)\n",
    "Average = np.mean(mergedData,axis=1)\n",
    "Variance = np.var(mergedData,axis=1)\n",
    "mergedData['Average'] = Average\n",
    "mergedData['Variance'] = Variance\n",
    "mergedData"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "source": [
    "## Determining Zones\n",
    "Here we first create 'averagedFrame's. These are dictionaries that at each 'label' (which corresponds to the name of an expirement) we have a pandas dataframe containing the results of all of the trails in an expirement summed, and then divided by the total number of trails.\n",
    "Anytime you are adjusting the Zones, everything below here must be run. The values of many of these DataFrames are mutated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preCursorFactor is defined at the start\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "    runSumFrames = expIndexes[label][0]-expIndexes[label][0]\n",
    "    for idx,time in enumerate(expIndexes[label]):\n",
    "        start = expIndexes[label][idx] - preCursorFactor\n",
    "        end = expIndexes[label][idx] + expTLen[label]\n",
    "        expirementFrame[label+' Exp '+str(idx+1)] = mergedData.iloc[ start : end , 1: ].reset_index(drop = True)\n",
    "        runSumFrames += expirementFrame[label+' Exp '+str(idx+1)]\n",
    "        \n",
    "    averagedFrame[label] = runSumFrames/(idx+1)"
   ]
  },
  {
   "source": [
    "Calculating the correct Zones for each expirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numberOfZones -= 1\n",
    "# numberOfZones is defined at the start\n",
    "AutoZoneAssignments = {}\n",
    "for frame in averagedFrame:\n",
    "    # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "    avgFrm = averagedFrame[frame]\n",
    "    # outdoorSensors must have its spelling exactly match\n",
    "    columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "    columns.sort()\n",
    "\n",
    "    X = {}\n",
    "    for column in columns:\n",
    "        value,index = max([(value,index) for index,value in enumerate(avgFrm[column])]) \n",
    "        X[column] = np.array([np.log(value+.01),index])\n",
    "    X = [X[i] for i in X]\n",
    "    kmeans = KMeans(n_clusters=numberOfZones,random_state=0).fit(X)\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(numberOfZones)\n",
    "    #lut = lut[::-1]\n",
    "    orderedZones = [[]]*numberOfZones\n",
    "    for index, zone in enumerate(lut):\n",
    "        orderedZones[index] = [index if zone == kmeans.labels_[i] else 0 for i in range(len(kmeans.labels_))]\n",
    "    AutoZoneAssignments[frame] = np.sum(orderedZones,axis=0)\n",
    "z = numberOfZones\n",
    "ZDfAuto = pd.DataFrame(AutoZoneAssignments)\n",
    "ZDfAuto = ZDfAuto.append(pd.DataFrame([[z]*len(expIndexes)]*len(outdoorSensors),columns = AutoZoneAssignments.keys()),ignore_index=True)\n",
    "AutoZoneAssignments = ZDfAuto\n",
    "# numberOfZones += 1\n",
    "\n",
    "if not ZoneAutomation:\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2]))\n",
    "        columns.sort()\n",
    "        ZoneAssignments[frame] = [0]*len(columns)\n",
    "        for value,zone in enumerate(zoneList):\n",
    "            for sensor in zoneList[zone]:\n",
    "                ZoneAssignments[frame][columns.index(sensor)] = value\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'ZoneAssignments.csv')\n",
    "ZDf.to_csv(location,index=False)\n",
    "\n",
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'AutoZoneAssignments.csv')\n",
    "ZDfAuto.to_csv(location,index=False)\n",
    "\n",
    "expirementFrameAuto = copy.deepcopy(expirementFrame)\n",
    "averagedFrameAuto = copy.deepcopy(averagedFrame)"
   ]
  },
  {
   "source": [
    "## Zoning the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "manual zones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'HMC Expirement 3 Exp 4'"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonedAvgFrame = {}\n",
    "for key in ZoneAssignments:\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    zonedAvgFrame[key] = averagedFrame[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedAvgFrame[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedAvgFrame[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrame = {}\n",
    "# create a list of all of the various dict keys in expirementFrame so that we can iterate through them to get the data\n",
    "labels = list(expirementFrame.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp ')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrame[exp] = expirementFrame[exp]\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedExpFrame[exp][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedExpFrame[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "        \n",
    "zonedAvgFrameAuto = {}\n",
    "for key in AutoZoneAssignments:\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    zonedAvgFrameAuto[key] = averagedFrameAuto[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedAvgFrameAuto[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedAvgFrameAuto[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "        \n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrameAuto = {}\n",
    "# create a list of all of the various dict keys in expirementFrameAuto so that we can iterate through them to get the data\n",
    "labels = list(expirementFrameAuto.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp ')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrameAuto[exp] = expirementFrameAuto[exp]\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedExpFrameAuto[exp][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedExpFrameAuto[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]"
   ]
  },
  {
   "source": [
    "auto zones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './averagedDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrameAuto:\n",
    "    temp=averagedFrameAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['B-01', 'B-02', 'B-04', 'B-05', 'B-06', 'B-07', 'B-08', 'B-09', 'B-10',\n",
       "       'B-11', 'B-12', 'B-13', 'B-14', 'B-15', 'B-16', 'B-17', 'B-18', 'B-19',\n",
       "       'B-20', 'B-21', 'B-22', 'B-23', 'Average', 'Variance', 'Zone 1',\n",
       "       'Zone 2', 'Zone 3'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)\n",
    "\n",
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns) \n",
    "stretchedDFAuto = {}\n",
    "for i in averagedFrameAuto:\n",
    "    tempFrame = averagedFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDFAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns) \n",
    "\n",
    "stretchExpDfAuto = {}\n",
    "for i in expirementFrameAuto:\n",
    "    tempFrame = expirementFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDfAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedAvgDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDFAuto:\n",
    "    temp=stretchedDFAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDfAuto:\n",
    "    temp=stretchExpDfAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16.463077306747437\n"
     ]
    }
   ],
   "source": [
    "end = clock.time()\n",
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}